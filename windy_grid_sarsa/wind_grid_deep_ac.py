'''
A place to explore simple implementations of deep AC.

This is crazy overkill for what could be handled by a simple tabular setup.

However, it's just to go through the motions, and for pedagogical purposes.

'''

import numpy as np
import matplotlib.pyplot as plt
import keras
import tensorflow as tf
from keras.models import Model, Sequential
from keras.layers import Dense
from keras.optimizers import SGD

tf.keras.backend.clear_session()


### pieces:
# environment: takes in state and action, emits new state, reward
# v-function: a deep model that maps states to values
# actor function: a deep model that maps state-action pairs to values

### flow:
# setup:
 # instantiate v-function and actor models
 # form a place to hold data generated by runs
 # code the environment
 # code a method for choosing actions given relative predicted values

# every step generates useful DATA to update the v and a models
# a state, an action, an observed reward, and a diff from the predicted reward
# save those things into a data structure...like...what?
# every so many iterations (how many?) update the v and a models on the basis of data
# that is, using half the data as held out, learn a bit until validation accuracy no longer falls...or something?
# for now, throw away all old data, and do this again.
# Later: intelligently keep around data on a smart schedule
# Later: make sure you get roughly the same *amount* of data, rather than relying on number of iterations...

# States are tuples (x,y)
# Actions are tuples (delta_x, delta_y)

### Environment
# takes a state and action, returns a state
# input [(state), (action)]   e.g. [(2,2),(1,0)]
# output (new state)   e.g (3,2)
def environment(state, action):
    # y 0-6
    # x 0-9
    # 'wind' is a function of beginning state
    # x {3, 4, 5, 8} add 1 to row
    # x {6,7} add 2 to row
    # start is (0,3)
    # goal is (7,3)

    # Check state. If wind, apply wind. If you're out-of-bounds, move that index in-bounds.
    one_wind = {3,4,5,8}
    two_wind = {6,7}

    x, y = state
    if x in one_wind:
        y += 1
    if x in two_wind:
        y += 2
    x += action[0]
    y += action[1]

    if x < 0:
        x = 0
    if x > 9:
        x = 9
    if y < 0:
        y=0
    if y > 6:
        y = 6

    out = (x,y)
    return(out)


# Function that maps states to values
v_function = Sequential()
v_function.add(Dense(4, activation='linear', input_shape=(2,)))
v_function.add(Dense(4, activation='linear'))
v_function.add(Dense(1, activation='linear'))

v_opt = keras.optimizers.Adam(learning_rate=0.01)
v_function.compile(optimizer=v_opt, loss='mean_squared_error', metrics=['accuracy'])


# Function that maps state-action pairs to values
a_function = Sequential()
a_function.add(Dense(4, activation='linear', input_shape=(4,)))
a_function.add(Dense(4, activation='linear'))
a_function.add(Dense(1, activation='linear'))

a_opt = keras.optimizers.Adam(learning_rate=0.01)
a_function.compile(optimizer=a_opt, loss='mean_squared_error', metrics=['accuracy'])


possible_actions = [(-1, 0), (0, 1), (1,0), (0, -1)]

def choose_action(actor_function, current_state, potential_actions):
    vals = []
    for i in range(len(possible_actions)):
        input_vector = np.array([current_state[0], current_state[1], possible_actions[i][0], possible_actions[i][1]])
        input_vector = input_vector.reshape((-1,4))
        val = a_function.predict(input_vector)[0][0]
        vals.append(val)
    prs = []
    for i in range(len(vals)):
        pr_i = np.exp(vals[i]) / np.sum([np.exp(vals[x]) for x in range(len(vals)) if x != i])
        prs.append(pr_i)
    prs = prs / np.sum(prs)
    choice_ind = np.random.choice(list(range(len(possible_actions))), size=1, replace=False, p=prs)[0]
    action = possible_actions[choice_ind]
    return action

def vec_state(state):
    x = np.array([state[0], state[1]])
    y = x.reshape(-1,2)
    return y



def episode(state_model, state_action_model, potential_actions, beta, iter,
            state_data_training, state_data_target, state_action_training, state_action_target):
    n_steps = 0
    iter = 0
    # initialize current state (start) and action(random)
    current_state = (0, 3)
    # Each episode you start, do updates, until goal.
    goal = False
    while goal != True:
        print(current_state)
        m1 = 'iter: ' + str(iter)
        print(m1)
        m2 = 'n_steps: ' + str(n_steps)
        print(m2)
        n_steps += 1
        action = choose_action(state_action_model, current_state, potential_actions)
        next_state = environment(current_state, action)
        curr_value = state_model.predict(vec_state(current_state))
        next_value = state_model.predict(vec_state(next_state))
        TD_error = -1 + next_value - curr_value

        # instead of updating tabular arrays here, we'll emit data to an outside locale
        actor_vector = np.array([current_state[0], current_state[1], action[0], action[1]])
        state_action_training[iter] = actor_vector
        actor_value = state_action_model.predict(actor_vector.reshape(-1,4))
        actor_target = actor_value + beta*TD_error
        state_action_target[iter] = actor_target

        state_vector = np.array([current_state[0], current_state[1]])
        state_data_training[iter] = state_vector
        state_value = state_model.predict(state_vector.reshape(-1,2))
        state_target = state_value + .1 * TD_error
        state_data_target[iter] = state_target

        if next_state == (7,3):
            goal=True
        current_state = next_state
        iter += 1

        if iter >= 40:
            return state_data_training, state_data_target, state_action_training, state_action_target

    return n_steps



state_data_training = np.zeros((1000, 2))
state_data_target = np.zeros((1000,))
state_action_training = np.zeros((1000, 4))
state_action_target = np.zeros((1000,))

# run one episode
episode(v_function, a_function, possible_actions, .1, 0, state_data_training, state_data_target, state_action_training, state_action_target)
'''
# update models
v_function.fit(x=state_data_training, y=state_data_target, epochs=2)
a_function.fit(x=state_action_training, y=state_action_target, epochs=2)
# reset data
state_data_training = np.zeros((1000, 2))
state_action_training = np.zeros((1000, 4))

'''


